{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4e1755-40ff-4c18-aefb-ecbe42b3cc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 21:56:08 WARN Utils: Your hostname, group-36-prj resolves to a loopback address: 127.0.0.1; using 192.168.2.107 instead (on interface ens3)\n",
      "25/03/07 21:56:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/07 21:56:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = (\n",
    "    SparkSession.builder\n",
    "    .master(\"spark://localhost:7077\")  # Updated to the detected worker IP\n",
    "    .appName(\"group_36_app\")  \n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)  \n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)  \n",
    "    .config(\"spark.shuffle.service.enabled\", False)  \n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\")  \n",
    "    .config(\"spark.executor.cores\", 2)  # Each executor gets 2 cores\n",
    "    .config(\"spark.executor.memory\", \"2g\")  # Set executor memory (adjust as needed)\n",
    "    .config(\"spark.driver.port\", 9999)  \n",
    "    .config(\"spark.blockManager.port\", 10005)  \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark Session Created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f928eadf-45b3-449a-b4ef-3afed8464287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result is: [2, 4, 6, 8, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data = spark_session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "result = data.map(lambda x: x * 2).collect()\n",
    "print(\"result is:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581ec3ef-947e-42ea-8da5-4d898fc53175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 21:56:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/03/07 21:58:19 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.2.107: Command exited with code 137\n",
      "25/03/07 21:58:20 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4) (192.168.2.107 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/03/07 21:58:20 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) (192.168.2.107 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+----+-----+------+\n",
      "|      date|   county|  state|fips|cases|deaths|\n",
      "+----------+---------+-------+----+-----+------+\n",
      "|2022-01-01|  Autauga|Alabama|1001|11018|   160|\n",
      "|2022-01-01|  Baldwin|Alabama|1003|39911|   593|\n",
      "|2022-01-01|  Barbour|Alabama|1005| 3860|    81|\n",
      "|2022-01-01|     Bibb|Alabama|1007| 4533|    95|\n",
      "|2022-01-01|   Blount|Alabama|1009|11256|   198|\n",
      "|2022-01-01|  Bullock|Alabama|1011| 1676|    46|\n",
      "|2022-01-01|   Butler|Alabama|1013| 3613|   102|\n",
      "|2022-01-01|  Calhoun|Alabama|1015|23411|   532|\n",
      "|2022-01-01| Chambers|Alabama|1017| 6171|   147|\n",
      "|2022-01-01| Cherokee|Alabama|1019| 3385|    65|\n",
      "|2022-01-01|  Chilton|Alabama|1021| 7424|   173|\n",
      "|2022-01-01|  Choctaw|Alabama|1023| 1026|    30|\n",
      "|2022-01-01|   Clarke|Alabama|1025| 4944|    88|\n",
      "|2022-01-01|     Clay|Alabama|1027| 2658|    69|\n",
      "|2022-01-01| Cleburne|Alabama|1029| 2649|    60|\n",
      "|2022-01-01|   Coffee|Alabama|1031| 9782|   195|\n",
      "|2022-01-01|  Colbert|Alabama|1033| 9822|   214|\n",
      "|2022-01-01|  Conecuh|Alabama|1035| 1983|    62|\n",
      "|2022-01-01|    Coosa|Alabama|1037| 1948|    48|\n",
      "|2022-01-01|Covington|Alabama|1039| 7139|   200|\n",
      "+----------+---------+-------+----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CovidDataProcessing\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"hdfs:///user/group36_project/dataset/us-counties-2022.csv\", header=True, inferSchema=True)\n",
    "df = df.filter(df[\"cases\"] > 1000)  # Filter data with case numbers greater than 1000\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fb7034-43ff-455e-90f0-565c5ab9dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 21:59:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/03/07 21:59:59 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.2.107: Command exited with code 137\n",
      "25/03/07 21:59:59 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 10) (192.168.2.107 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/03/07 21:59:59 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 9) (192.168.2.107 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/03/07 22:01:21 ERROR TaskSchedulerImpl: Lost executor 3 on 192.168.2.107: Command exited with code 137\n",
      "25/03/07 22:01:21 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 14) (192.168.2.107 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/03/07 22:01:21 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 13) (192.168.2.107 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "[Stage 8:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases in Florida: 138123133\n",
      "Execution time: 168.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"FloridaCovidCases\").getOrCreate()\n",
    "\n",
    "# read data on HDFS\n",
    "file_path = \"hdfs:///user/group36_project/dataset/us-counties-2020.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Counting cases in Florida\n",
    "florida_cases = df.filter(df[\"state\"] == \"Florida\").groupBy().sum(\"cases\").collect()[0][0]\n",
    "\n",
    "# record finish time\n",
    "end_time = time.time()\n",
    "\n",
    "# compute running time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# output the result\n",
    "print(f\"Total cases in Florida: {florida_cases}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22ea470-dad4-49f8-86db-a167839f4270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 22:03:02 ERROR TaskSchedulerImpl: Lost executor 4 on 192.168.2.107: Command exited with code 137\n",
      "25/03/07 22:03:02 WARN TaskSetManager: Lost task 1.0 in stage 17.0 (TID 35) (192.168.2.107 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/03/07 22:03:02 WARN TaskSetManager: Lost task 0.0 in stage 17.0 (TID 34) (192.168.2.107 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "[Stage 22:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases in Florida: 138123133\n",
      "Execution time: 104.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"FloridaCovidCases\").getOrCreate()\n",
    "\n",
    "# read data on HDFS\n",
    "file_path = \"hdfs:///user/group36_project/dataset/us-counties-2020.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Counting cases in Florida\n",
    "df = df.repartition(10).filter(df[\"state\"] == \"Florida\")\n",
    "florida_cases = df.groupBy().sum(\"cases\").collect()[0][0]\n",
    "\n",
    "\n",
    "florida_cases = df.filter(df[\"state\"] == \"Florida\").groupBy().sum(\"cases\").collect()[0][0]\n",
    "\n",
    "# record finish time\n",
    "end_time = time.time()\n",
    "\n",
    "# compute running time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# output the result\n",
    "print(f\"Total cases in Florida: {florida_cases}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63818c8-1218-4f4d-a9ac-df62d49f4f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=============================>                            (1 + 1) / 2]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"FloridaCovidCases\").getOrCreate()\n",
    "\n",
    "# read data on HDFS\n",
    "file_path = \"hdfs:///user/group36_project/dataset/us-counties-2020.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Counting cases in Florida\n",
    "df = df.repartition(2).filter(df[\"state\"] == \"Florida\")\n",
    "florida_cases = df.groupBy().sum(\"cases\").collect()[0][0]\n",
    "\n",
    "\n",
    "florida_cases = df.filter(df[\"state\"] == \"Florida\").groupBy().sum(\"cases\").collect()[0][0]\n",
    "\n",
    "# record finish time\n",
    "end_time = time.time()\n",
    "\n",
    "# compute running time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# output the result\n",
    "print(f\"Total cases in Florida: {florida_cases}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38045018-e784-48ba-8fd7-2ce73ec287c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"FloridaCovidCases\").getOrCreate()\n",
    "\n",
    "# read data on HDFS\n",
    "file_path = \"hdfs:///user/group36_project/dataset/us-counties-2020.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Counting cases in Florida\n",
    "df = df.repartition(20).filter(df[\"state\"] == \"Florida\")\n",
    "florida_cases = df.groupBy().sum(\"cases\").collect()[0][0]\n",
    "\n",
    "\n",
    "florida_cases = df.filter(df[\"state\"] == \"Florida\").groupBy().sum(\"cases\").collect()[0][0]\n",
    "\n",
    "# record finish time\n",
    "end_time = time.time()\n",
    "\n",
    "# compute running time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# output the result\n",
    "print(f\"Total cases in Florida: {florida_cases}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e5c73-c3c6-455d-89c0-591c04ca9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"FloridaCovidCases\").getOrCreate()\n",
    "\n",
    "# read data on HDFS\n",
    "file_path = \"hdfs:///user/group36_project/dataset/us-counties-2020.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Counting cases in Florida\n",
    "df = df.repartition(1).filter(df[\"state\"] == \"Florida\")\n",
    "florida_cases = df.groupBy().sum(\"cases\").collect()[0][0]\n",
    "\n",
    "\n",
    "florida_cases = df.filter(df[\"state\"] == \"Florida\").groupBy().sum(\"cases\").collect()[0][0]\n",
    "\n",
    "# record finish time\n",
    "end_time = time.time()\n",
    "\n",
    "# compute running time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# output the result\n",
    "print(f\"Total cases in Florida: {florida_cases}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c3606-42c9-4f48-9104-2abb47666a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CheckBlockSize\").getOrCreate()\n",
    "\n",
    "spark_block_size = spark.conf.get(\"spark.sql.files.maxPartitionBytes\", \"134217728\")\n",
    "print(f\"Spark Block Size: {int(spark_block_size) / (1024 * 1024)} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93f1b4-c73a-4402-87b0-98ff653000d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58541fe9-8796-460f-be75-6e833f638c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
